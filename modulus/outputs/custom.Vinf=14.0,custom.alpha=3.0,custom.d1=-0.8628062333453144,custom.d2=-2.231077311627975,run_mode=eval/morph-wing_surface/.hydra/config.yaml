training:
  max_steps: 100000
  grad_agg_freq: 1
  rec_results_freq: 1000
  rec_validation_freq: 5000
  rec_inference_freq: ${training.rec_results_freq}
  rec_monitor_freq: ${training.rec_results_freq}
  rec_constraint_freq: 2000
  save_network_freq: 1000
  print_stats_freq: 100
  summary_freq: 1000
  amp: false
  amp_dtype: float16
  ntk:
    use_ntk: false
    save_name: null
    run_freq: 1000
graph:
  func_arch: false
  func_arch_allow_partial_hessian: true
stop_criterion:
  metric: null
  min_delta: null
  patience: 50000
  mode: min
  freq: 1000
  strict: false
profiler:
  profile: false
  start_step: 0
  end_step: 100
  name: nvtx
network_dir: .
initialization_network_dir: ''
save_filetypes: npy
summary_histograms: false
jit: true
jit_use_nvfuser: true
jit_arch_mode: only_activation
jit_autograd_nodes: false
cuda_graphs: true
cuda_graph_warmup: 20
find_unused_parameters: false
broadcast_buffers: false
device: ''
debug: false
run_mode: eval
arch:
  branch_tau:
    arch_type: fully_connected
    input_keys: ???
    output_keys: ???
    detach_keys: ???
    scaling: null
    layer_size: 128
    nr_layers: 4
    skip_connections: false
    activation_fn: silu
    adaptive_activations: false
    weight_norm: true
  branch_p:
    arch_type: fully_connected
    input_keys: ???
    output_keys: ???
    detach_keys: ???
    scaling: null
    layer_size: 128
    nr_layers: 4
    skip_connections: false
    activation_fn: silu
    adaptive_activations: false
    weight_norm: true
  trunk:
    arch_type: fully_connected
    input_keys: ???
    output_keys: ???
    detach_keys: ???
    scaling: null
    layer_size: 128
    nr_layers: 4
    skip_connections: false
    activation_fn: silu
    adaptive_activations: false
    weight_norm: true
  deeponet_tau:
    arch_type: deeponet
    input_keys: ???
    output_keys: tau_x, tau_y, tau_z
    detach_keys: ???
    scaling: null
    trunk_dim: null
    branch_dim: null
  deeponet_p:
    arch_type: deeponet
    input_keys: ???
    output_keys: p
    detach_keys: ???
    scaling: null
    trunk_dim: null
    branch_dim: null
models: ???
loss:
  _target_: modulus.sym.loss.aggregator.Sum
  weights: null
optimizer:
  _params_:
    compute_gradients: adam_compute_gradients
    apply_gradients: adam_apply_gradients
  _target_: torch.optim.Adam
  lr: 0.001
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 0.0
  amsgrad: false
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  T_0: 15000
  T_mult: 2
  eta_min: 0.0001
  last_epoch: -1
batch_size:
  train: 7000
  lr_phys: 500
  validation: 3000
custom:
  d1: -0.8628062333453144
  d2: -2.231077311627975
  Vinf: 14.0
  alpha: 3.0
